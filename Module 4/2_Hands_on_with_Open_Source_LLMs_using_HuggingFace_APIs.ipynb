{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUwrTGNs4kSN"
   },
   "source": [
    "<center> <h1> Prompt Engineering with Open-Source Large Language Models (LLMs) using HuggingFace Serverless APIs</h1> </center>\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGwBgKICV_DJ"
   },
   "source": [
    "\n",
    "In this notebook we will learn how to run any open-source LLMs via HugginFace Inference APIs using this colab notebook. You can run this notebook in your local server also without worrying about having enough infrastructure to run these models!\n",
    "\n",
    "Thankfully HuggingFace has made its [__Inference API__](https://huggingface.co/docs/api-inference/quicktour) free to use with some basic rate limits etc. in place so you don't end up making unlimited requests on it's servers.\n",
    "\n",
    "The best part is you can access 150,000+ deep learning models without worrying about your infrastructure.\n",
    "\n",
    "The models we will be trying here include:\n",
    "\n",
    "- __[Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)__ model which is a 7B parameters transformer LLM built by the French young company [MistralAI](https://mistral.ai/company/)  is a instruct fine-tuned version of the [Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) which is based on their first [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) generative text model using a variety of publicly available conversation datasets.\n",
    "\n",
    "- __[gemma-2b-it ](https://huggingface.co/google/gemma-1.1-2b-it)__ is a part of Google's gemma series, a 2 billion parameter transformer model fine-tuned for instruction-following tasks, enabling it to handle a wide array of complex language processing activities.\n",
    "\n",
    "\n",
    "\n",
    "__You just need an internet connection and a HuggingFace Account and API Key to use these models.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w81pr0vzhNaJ"
   },
   "source": [
    "## Get your API Key\n",
    "\n",
    "Remember to go to your [HuggingFace Account Settings](https://huggingface.co/settings/account) and generate an API key by creating a new token from the [Access Tokens](https://huggingface.co/settings/tokens) section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGSuxLmAiCGP"
   },
   "source": [
    "## Load HuggingFace API Credentials\n",
    "\n",
    "Enter your key from [here](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_YjkI6sVG7H",
    "outputId": "0926504d-d4e0-4351-9ff9-8f5a47dc79e8"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter HuggingFace API Key:  ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "API_KEY = getpass(\"Enter HuggingFace API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgZzsDCZiN3k"
   },
   "source": [
    "### Create LLM API Access Function\n",
    "\n",
    "Here we create a basic function which can access any LLM API endpoint available on HuggingFace.\n",
    "\n",
    "For more details refer to the [detailed documentation](https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task) as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-TRno7hBB_qX"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer \"+API_KEY}\n",
    "\n",
    "def query(payload, MODEL_API_URL):\n",
    "  response = requests.post(MODEL_API_URL, headers=headers, json=payload)\n",
    "  print('API Response:', response)\n",
    "  return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BPGezswkYNZ"
   },
   "source": [
    "## Create LLM API Access Config\n",
    "\n",
    "Here we decide which LLMs we will access by getting their inference API endpoints.\n",
    "\n",
    "We also set some general configuration settings. You can find the [detailed documentation](https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task) here.\n",
    "\n",
    "Some useful config settings include:\n",
    "\n",
    "- max_new_tokens: The amount of new tokens to be generated in the response\n",
    "- do_sample: Whether or not to use sampling. False means use greedy decoding i.e temperature=0\n",
    "- temperature: Between 0 - 1, The value used to module the next token probabilities. Higher temperature means the results may vary and be more creative\n",
    "- return_full_text: If set to False, does not return your input prompt to the model\n",
    "- wait_for_model:  If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done\n",
    "- repetition_penalty: The more a token is used within generation the more it is penalized to not be picked in successive generation passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WtpZr9DlXHPZ"
   },
   "outputs": [],
   "source": [
    "MISTRAL7B_API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "mistral_params = {\n",
    "                  \"wait_for_model\": True,\n",
    "                  \"do_sample\": False,\n",
    "                  \"return_full_text\": False,\n",
    "                  \"max_new_tokens\": 1000,\n",
    "                }\n",
    "\n",
    "GEMMA2B_IT_API_URL = \"https://api-inference.huggingface.co/models/google/gemma-1.1-2b-it\"\n",
    "gemma_params = {\n",
    "                    \"wait_for_model\": True,\n",
    "                    \"do_sample\": False,\n",
    "                    \"return_full_text\": False,\n",
    "                    \"max_new_tokens\": 1000,\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "repJ_4TCllJO"
   },
   "source": [
    "## Prompting with Open-Source LLM APIs\n",
    "\n",
    "Now we will use HugginFace LLM APIs and try some tasks with prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjD6KmrBluQE"
   },
   "source": [
    "### 1. Basic Q & A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JIyiS1TtYfMi",
    "outputId": "7239137a-9783-4b41-db92-47be248df2af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you explain what is quantum computing to a 5th grader?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Can you explain what is quantum computing to a 5th grader?\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "fjo2tQmVjoKF",
    "outputId": "a761608d-5b9f-4925-eb6c-dba5791594b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MISTRAL7B_API_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9y8uhKXOB7B",
    "outputId": "82746635-a155-48aa-9b4f-a1e3f5bf03fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: <Response [200]>\n",
      "\n",
      "\n",
      "Quantum computing is like having a super-smart magic box that can do lots of calculations at the same time. Instead of using regular bits like a computer you know, it uses something called \"quantum bits\" or \"qubits.\" A qubit can be both 0 and 1 at the same time, which is like having a coin that is heads and tails at the same time! This makes quantum computers really fast at solving certain kinds of problems. But it's also very hard to build and keep them working, so we're still figuring out how to use them best.\n"
     ]
    }
   ],
   "source": [
    "output = query(payload={\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": mistral_params\n",
    "                },\n",
    "                MODEL_API_URL=MISTRAL7B_API_URL)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3cJytUzYc4I",
    "outputId": "f5b5fcb7-9faf-4e25-85be-c2fd150f8776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: <Response [200]>\n",
      "\n",
      "\n",
      "Imagine you have a coin. A regular coin can only be heads or tails. But a special kind of coin called a quantum coin can be both heads and tails at the same time!\n",
      "\n",
      "Quantum computing is like using these special coins to do calculations. Instead of regular bits that are either 0 or 1, quantum computers use qubits, which can be both 0 and 1 at the same time.\n",
      "\n",
      "This allows them to solve problems that are too complex for regular computers. For example, they can help us find the best route for a delivery truck or design new medicines.\n",
      "\n",
      "Quantum computing is still very new, but it has the potential to change the world in many ways.\n"
     ]
    }
   ],
   "source": [
    "# with Gemma\n",
    "output = query(payload={\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": gemma_params\n",
    "                },\n",
    "                MODEL_API_URL=GEMMA2B_IT_API_URL)\n",
    "response = output[0]['generated_text']\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Imagine you have a coin. A regular coin can only be heads or tails. But a special kind of coin called a quantum coin can be both heads and tails at the same time!\n",
       "\n",
       "Quantum computing is like using these special coins to do calculations. Instead of regular bits that are either 0 or 1, quantum computers use qubits, which can be both 0 and 1 at the same time.\n",
       "\n",
       "This allows them to solve problems that are too complex for regular computers. For example, they can help us find the best route for a delivery truck or design new medicines.\n",
       "\n",
       "Quantum computing is still very new, but it has the potential to change the world in many ways."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lD2_UpD_l5CA"
   },
   "source": [
    "### 2. Report Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r88x6sQWCheK",
    "outputId": "7815782d-566f-4840-8f00-3babdfd96990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize the following report delimited by triple backticks on Generative AI in max 5 lines\n",
      "\n",
      "Report:\n",
      "```\n",
      "Generative AI is a type of artificial intelligence technology that can produce various types of content, including text, imagery, audio and synthetic data. The recent buzz around generative AI has been driven by the simplicity of new user interfaces for creating high-quality text, graphics and videos in a matter of seconds.\n",
      "The technology, it should be noted, is not brand-new. Generative AI was introduced in the 1960s in chatbots. But it was not until 2014, with the introduction of generative adversarial networks, or GANs -- a type of machine learning algorithm -- that generative AI could create convincingly authentic images, videos and audio of real people.\n",
      "On the one hand, this newfound capability has opened up opportunities that include better movie dubbing and rich educational content. It also unlocked concerns about deepfakes -- digitally forged images or videos -- and harmful cybersecurity attacks on businesses, including nefarious requests that realistically mimic an employee's boss.\n",
      "Two additional recent advances that will be discussed in more detail below have played a critical part in generative AI going mainstream: transformers and the breakthrough language models they enabled. Transformers are a type of machine learning that made it possible for researchers to train ever-larger models without having to label all of the data in advance. New models could thus be trained on billions of pages of text, resulting in answers with more depth. In addition, transformers unlocked a new notion called attention that enabled models to track the connections between words across pages, chapters and books rather than just in individual sentences. And not just words: Transformers could also use their ability to track connections to analyze code, proteins, chemicals and DNA.\n",
      "The rapid advances in so-called large language models (LLMs) -- i.e., models with billions or even trillions of parameters -- have opened a new era in which generative AI models can write engaging text, paint photorealistic images and even create somewhat entertaining sitcoms on the fly. Moreover, innovations in multimodal AI enable teams to generate content across multiple types of media, including text, graphics and video. This is the basis for tools like Dall-E that automatically create images from a text description or generate text captions from images.\n",
      "These breakthroughs notwithstanding, we are still in the early days of using generative AI to create readable text and photorealistic stylized graphics. Early implementations have had issues with accuracy and bias, as well as being prone to hallucinations and spitting back weird answers. Still, progress thus far indicates that the inherent capabilities of this generative AI could fundamentally change enterprise technology how businesses operate. Going forward, this technology could help write code, design new drugs, develop products, redesign business processes and transform supply chains.\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = \"\"\"\n",
    "Generative AI is a type of artificial intelligence technology that can produce various types of content, including text, imagery, audio and synthetic data. The recent buzz around generative AI has been driven by the simplicity of new user interfaces for creating high-quality text, graphics and videos in a matter of seconds.\n",
    "The technology, it should be noted, is not brand-new. Generative AI was introduced in the 1960s in chatbots. But it was not until 2014, with the introduction of generative adversarial networks, or GANs -- a type of machine learning algorithm -- that generative AI could create convincingly authentic images, videos and audio of real people.\n",
    "On the one hand, this newfound capability has opened up opportunities that include better movie dubbing and rich educational content. It also unlocked concerns about deepfakes -- digitally forged images or videos -- and harmful cybersecurity attacks on businesses, including nefarious requests that realistically mimic an employee's boss.\n",
    "Two additional recent advances that will be discussed in more detail below have played a critical part in generative AI going mainstream: transformers and the breakthrough language models they enabled. Transformers are a type of machine learning that made it possible for researchers to train ever-larger models without having to label all of the data in advance. New models could thus be trained on billions of pages of text, resulting in answers with more depth. In addition, transformers unlocked a new notion called attention that enabled models to track the connections between words across pages, chapters and books rather than just in individual sentences. And not just words: Transformers could also use their ability to track connections to analyze code, proteins, chemicals and DNA.\n",
    "The rapid advances in so-called large language models (LLMs) -- i.e., models with billions or even trillions of parameters -- have opened a new era in which generative AI models can write engaging text, paint photorealistic images and even create somewhat entertaining sitcoms on the fly. Moreover, innovations in multimodal AI enable teams to generate content across multiple types of media, including text, graphics and video. This is the basis for tools like Dall-E that automatically create images from a text description or generate text captions from images.\n",
    "These breakthroughs notwithstanding, we are still in the early days of using generative AI to create readable text and photorealistic stylized graphics. Early implementations have had issues with accuracy and bias, as well as being prone to hallucinations and spitting back weird answers. Still, progress thus far indicates that the inherent capabilities of this generative AI could fundamentally change enterprise technology how businesses operate. Going forward, this technology could help write code, design new drugs, develop products, redesign business processes and transform supply chains.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following report delimited by triple backticks on Generative AI in max 5 lines\n",
    "\n",
    "Report:\n",
    "```{report}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Pi9uzJ8dObA",
    "outputId": "80750f8c-1278-4355-da17-5f6b8c1f7f70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: <Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Generative AI is a technology that can create various types of content, including text, images, audio, and synthetic data. Introduced in the 1960s, it gained popularity with the introduction of generative adversarial networks (GANs) in 2014, enabling the creation of convincing images, videos, and audio. Recent advances in transformers and large language models have led to the generation of engaging text and photorealistic graphics, with potential applications in enterprise technology such as code writing, drug development, and supply chain transformation. However, challenges remain, including accuracy, bias, and hallucinations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = query(payload={\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": mistral_params\n",
    "                },\n",
    "                MODEL_API_URL=MISTRAL7B_API_URL)\n",
    "\n",
    "response = output[0]['generated_text']\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9QqGVIQdTMD",
    "outputId": "de0ef3c0-a09c-457a-f805-ec666b80397e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: <Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Generative AI is a rapidly evolving field with the potential to revolutionize various industries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with Gemma\n",
    "output = query(payload={\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": gemma_params\n",
    "                },\n",
    "                MODEL_API_URL=GEMMA2B_IT_API_URL)\n",
    "response = output[0]['generated_text']\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJTyaa37lvz9"
   },
   "source": [
    "### 3. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4ZKsd_3HaOw_"
   },
   "outputs": [],
   "source": [
    "review = \"\"\"I recently worked with this real estate company to purchase my first home,\n",
    "    and the experience was outstanding. The agent was knowledgeable, patient, and incredibly responsive.\n",
    "    They guided me through every step of the process, making what could have been a stressful\n",
    "    experience very smooth and enjoyable.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zA6_Ayr7aR3R",
    "outputId": "3c35337b-fbe5-41ae-f989-532ff7410297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: <Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sentiment: Positive\n",
       "\n",
       "Key Topics or Phrases:\n",
       "1. Outstanding experience\n",
       "2. Knowledgeable agent\n",
       "3. Patient and incredibly responsive\n",
       "4. Smooth and enjoyable process\n",
       "5. Guided through every step."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Act as a customer review analyst, given the following customer review text,\n",
    "do the following tasks:\n",
    "- Find the sentiment (positive, negative or neutral)\n",
    "- Extract max 5 key topics or phrases of the good or bad in the review\n",
    "Review Text:\n",
    "{review}\n",
    "\"\"\"\n",
    "\n",
    "mistral_output = query(payload={\n",
    "              \"inputs\": prompt,\n",
    "              \"parameters\": mistral_params\n",
    "              },\n",
    "              MODEL_API_URL=MISTRAL7B_API_URL)\n",
    "\n",
    "response = mistral_output[0]['generated_text']\n",
    "display(Markdown(response))\n",
    "# print(mistral_output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0ID9ysnFKEz",
    "outputId": "dd2b6bab-5849-4d40-e1f4-b73f6dac57e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: <Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "    The only downside was the high price of the property.\n",
       "\n",
       "Overall, I would rate my experience with this real estate company as 4 out of 5 stars.\n",
       "\n",
       "**Sentiment:**\n",
       "The sentiment of the review is positive. The customer is expressing satisfaction with the service provided by the real estate agent and the overall experience.\n",
       "\n",
       "**Key Topics:**\n",
       "1. Excellent agent knowledge and patience\n",
       "2. Smooth and enjoyable process\n",
       "3. High price of the property\n",
       "4. Responsive agent\n",
       "5. Overall positive experience"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with Gemma\n",
    "gemma_output = query(payload={\n",
    "              \"inputs\": prompt,\n",
    "              \"parameters\": gemma_params\n",
    "              },\n",
    "              MODEL_API_URL=GEMMA2B_IT_API_URL)\n",
    "\n",
    "response = gemma_output[0]['generated_text']\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yx-K95jwGcBD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
